{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQcuUsVyUykc/fTIPudNbE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nissysathwika/Anamoly-detection/blob/main/AutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = pd.read_csv('transaction.csv')"
      ],
      "metadata": {
        "id": "DsvJOB9ec4O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values for numeric columns only\n",
        "for col in data.select_dtypes(include=np.number):\n",
        "    data[col] = data[col].fillna(data[col].median())\n",
        "\n",
        "# Handle missing values for non-numeric columns (e.g., using mode or a constant)\n",
        "for col in data.select_dtypes(exclude=np.number):\n",
        "    # Example: fill with the most frequent value (mode)\n",
        "    data[col] = data[col].fillna(data[col].mode()[0])"
      ],
      "metadata": {
        "id": "AS_e6BCY9pSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Timestamp to datetime\n",
        "data['Timestamp'] = pd.to_datetime(data['Timestamp'], errors='coerce')\n",
        "data.dropna(subset=['Timestamp'], inplace=True)  # Drop rows where conversion failed\n",
        "\n",
        "# Extract useful features from Timestamp\n",
        "data['Hour'] = data['Timestamp'].dt.hour\n",
        "data['DayOfWeek'] = data['Timestamp'].dt.dayofweek"
      ],
      "metadata": {
        "id": "Qeu6GpGl97OF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns that are not useful for classification (e.g., Timestamp, TransactionID)\n",
        "data.drop(columns=['Timestamp', 'TransactionID'], inplace=True)\n"
      ],
      "metadata": {
        "id": "Zr4njE4N9-c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdg64QMR-Bcg",
        "outputId": "7f1e106f-2087-4c2d-e09e-301fa94e90c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['AccountID', 'Amount', 'Merchant', 'TransactionType', 'Location',\n",
            "       'Hour', 'DayOfWeek'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding for categorical columns (AccountID, Merchant, TransactionType, Location)\n",
        "data_encoded = pd.get_dummies(data, columns=['AccountID', 'Merchant', 'TransactionType', 'Location'], drop_first=True)\n",
        "\n",
        "# Features (X) and target variable (y - assuming Amount is indicative of anomalies)\n",
        "X = data_encoded.drop(columns=['Amount'])\n",
        "y = data_encoded['Amount']"
      ],
      "metadata": {
        "id": "eYMt2-IU-FcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Feature Selection using RFE\n",
        "# Using Linear Regression as a base estimator for RFE\n",
        "from sklearn.linear_model import LinearRegression # Import LinearRegression\n",
        "model = LinearRegression() # Changed model to LinearRegression\n",
        "rfe = RFE(model, n_features_to_select=10)  # Select top 10 features\n",
        "fit = rfe.fit(X, y)"
      ],
      "metadata": {
        "id": "rOwZzEpV-IvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selected features\n",
        "selected_features = X.columns[fit.support_]\n",
        "print(\"Selected Features: \", selected_features)\n",
        "\n",
        "# Update X to use only selected features\n",
        "X_selected = X[selected_features]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yibdp6EO-SmS",
        "outputId": "945a719a-5d25-4bf2-8931-0d5be3af2110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features:  Index(['AccountID_ACC10', 'AccountID_ACC11', 'AccountID_ACC13',\n",
            "       'AccountID_ACC5', 'AccountID_ACC9', 'Merchant_MerchantE',\n",
            "       'Merchant_MerchantF', 'Location_Los Angeles', 'Location_New York',\n",
            "       'Location_San Francisco'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Data Preprocessing - Normalize the selected features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_selected)\n"
      ],
      "metadata": {
        "id": "ThRJBIPq-XDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "X_train, X_test = train_test_split(X_scaled, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "5byXi1sG-LBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Build the Autoencoder model\n",
        "input_dim = X_train.shape[1]  # Input size is the number of selected features\n",
        "encoding_dim = 8  # Number of nodes in the bottleneck layer"
      ],
      "metadata": {
        "id": "ZQxAp_FM9qhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Autoencoder architecture\n",
        "autoencoder = models.Sequential([\n",
        "    layers.Input(shape=(input_dim,)),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(encoding_dim, activation='relu'),  # Bottleneck layer\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(input_dim, activation='linear')  # Output layer for reconstruction\n",
        "])"
      ],
      "metadata": {
        "id": "ihKf2noO-tVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "autoencoder.compile(optimizer='adam', loss='mse')"
      ],
      "metadata": {
        "id": "D5guF2NP-uxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Autoencoder\n",
        "history = autoencoder.fit(X_train, X_train, epochs=50, batch_size=256, validation_data=(X_test, X_test), verbose=0)"
      ],
      "metadata": {
        "id": "jFyxCDnD-xgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Predict on the test data (reconstruction)\n",
        "X_train_pred = autoencoder.predict(X_train)\n",
        "X_test_pred = autoencoder.predict(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq0HW7L_-0NS",
        "outputId": "c419a77e-2249-4824-c83c-7e8e15010ed4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m420/420\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute reconstruction error for train and test data\n",
        "train_reconstruction_error = np.mean(np.square(X_train - X_train_pred), axis=1)\n",
        "test_reconstruction_error = np.mean(np.square(X_test - X_test_pred), axis=1)"
      ],
      "metadata": {
        "id": "rwSXorrY-7ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Set a threshold for anomalies based on training reconstruction error\n",
        "threshold = np.mean(train_reconstruction_error) + 3 * np.std(train_reconstruction_error)"
      ],
      "metadata": {
        "id": "19ndCp_0-85b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify anomalies in the test set\n",
        "anomalies = test_reconstruction_error > threshold"
      ],
      "metadata": {
        "id": "Lwrct6av_BFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of anomalies\n",
        "anomaly_count = np.sum(anomalies)\n",
        "\n",
        "print(f\"Number of anomalies detected: {anomaly_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdFJ2xpI_tWC",
        "outputId": "d3f73df9-766d-4976-fde5-e41515119766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of anomalies detected: 101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For labeling purposes, create a binary classification: 1 for anomalies, 0 for normal\n",
        "y_test_pred = np.array([1 if e > threshold else 0 for e in test_reconstruction_error])"
      ],
      "metadata": {
        "id": "xJoy9xJG_EVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we don't have actual labels, assume normal (0) for the majority\n",
        "y_true = np.zeros_like(y_test_pred)"
      ],
      "metadata": {
        "id": "y1BhszE4_H4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import recall_score\n",
        "\n",
        "# Calculate recall with zero_division set to 0\n",
        "recall = recall_score(y_true, y_test_pred, average='macro', zero_division=0) # Use y_test_pred instead of y_pred"
      ],
      "metadata": {
        "id": "19hK3xws_awl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Calculate accuracy and print classification report\n",
        "accuracy = accuracy_score(y_true, y_test_pred)\n",
        "report = classification_report(y_true, y_test_pred, target_names=[\"Normal\", \"Anomaly\"])\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDDueYF1_JR7",
        "outputId": "dedab905-8eaa-4de1-a5dc-a27d4e2287cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9699404761904762\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       1.00      0.97      0.98      3360\n",
            "     Anomaly       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.97      3360\n",
            "   macro avg       0.50      0.48      0.49      3360\n",
            "weighted avg       1.00      0.97      0.98      3360\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ]
}